{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis as a dimensionality reduction technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.stats as stats\n",
    "%matplotlib notebook\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Activity Question 1 (C)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-08116d844484>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-08116d844484>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    J_a = #(w_a.T @(mu_0 - mu_1))**2 / (w_a.T@S@w_a)\u001b[0m\n\u001b[1;37m                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mu_0 = np.array([3,3.67]).reshape((2,1))\n",
    "mu_1 = np.array([4.67,2]).reshape((2,1))\n",
    "w_a = np.array([10,-1]).reshape((2,1))\n",
    "w_b = np.array([-1.1,1]).reshape((2,1))\n",
    "w_c = np.array([0.2,1]).reshape((2,1))\n",
    "w_d = np.array([0.9,1]).reshape((2,1))\n",
    "S = np.array([[15.33,9],[9,13.2]])\n",
    "J_a = # fill in your code here\n",
    "J_b = # fill in your code here\n",
    "J_c = # fill in your code here\n",
    "J_d = # fill in your code here\n",
    "print(\"J_a:\", J_a)\n",
    "print(\"J_b:\", J_b)\n",
    "print(\"J_c:\", J_c)\n",
    "print(\"J_d:\", J_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_a = (w_a.T @(mu_0 - mu_1))**2 / (w_a.T@S@w_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Activity Question 2\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data. The \"random_state\" was set to 2 for easy replication \n",
    "x_p2, y_p2 = datasets.make_classification(n_samples=150, n_features=3, n_informative=2, n_redundant=0, \n",
    "                                          n_repeated=0, n_classes=3, n_clusters_per_class=1, weights=None, \n",
    "                                          class_sep=2, random_state=2, scale = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the generated data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(xs=x_p2[:,0][y_p2 == 0], ys=x_p2[:,1][y_p2 == 0], zs=x_p2[:,2][y_p2 == 0],  \n",
    "           marker='s', color='red', alpha=0.5, label=\"class 0\")\n",
    "ax.scatter(xs=x_p2[:,0][y_p2 == 1], ys=x_p2[:,1][y_p2 == 1], zs=x_p2[:,2][y_p2 == 1],  \n",
    "           marker='o', color='blue', alpha=0.5, label=\"class 1\")\n",
    "ax.scatter(xs=x_p2[:,0][y_p2 == 2], ys=x_p2[:,1][y_p2 == 2], zs=x_p2[:,2][y_p2 == 2],  \n",
    "           marker='^', color='green', alpha=0.5, label=\"class 2\")\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$x_3$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data according to different classes. \n",
    "# note that the classes are {0,1,2}, not {1,2,3}\n",
    "x_p2_c0 = x_p2[:,:][y_p2 == 0]\n",
    "x_p2_c1 = x_p2[:,:][y_p2 == 1]\n",
    "x_p2_c2 = x_p2[:,:][y_p2 == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: calculate the mean of the each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_c0 = # fill in your code here\n",
    "mu_c1 = # fill in your code here\n",
    "mu_c2 = # fill in your code here\n",
    "mu = np.mean(x_p2, axis=0)\n",
    "n_0 = x_p2_c0.shape[0]\n",
    "n_1 = x_p2_c1.shape[0]\n",
    "n_2 = x_p2_c2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mu_c0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-220f95f892e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mean of class 0 is\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmu_c0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\nMean of class 1 is\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu_c1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\nMean of class 2 is\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu_c2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\nMean of all data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mu_c0' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Mean of class 0 is\",mu_c0, \"\\nMean of class 1 is\", mu_c1, \"\\nMean of class 2 is\", mu_c2, \"\\nMean of all data\", mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate the Between-Class Variance matrix $S_B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the formula to calculate $S_B$ is:\n",
    "$$S_{B} = \\sum_{j=0}^{k-1}n_j(\\vec{\\mu_j}-\\vec{\\mu})(\\vec{\\mu_j}-\\vec{\\mu})^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate S_B_j for each class. Then add together\n",
    "# Hint: use reshape((m,1)) to convert a 1-dimensional numpy array to a 2-dimensional numpy array of shape (m,1)\n",
    "S_B_0 = # fill in your code here\n",
    "S_B_1 = # fill in your code here\n",
    "S_B_2 = # fill in your code here\n",
    "S_B = n_0 * S_B_0 + n_1 * S_B_1 + n_2 * S_B_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate the Within-Class Variance matrix $S_W$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the formula to calculate $S_W$ is:\n",
    "$$S_{W_j} = \\sum_{\\vec{x}^{(i)} \\in C_j}(\\vec{x}^{(i)}-\\vec{\\mu_j})(\\vec{x}^{(i)}-\\vec{\\mu_j})^T,\\ j=0,1,2$$\n",
    "$$S_W = \\sum_{j = 0}^{2}S_{W_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate S_W_j for each class j. S_W is the sum of all S_W_j\n",
    "S_W_0 = (x_p2_c0 - mu_c0).T@(x_p2_c0 - mu_c0)\n",
    "S_W_1 = (x_p2_c1 - mu_c1).T@(x_p2_c1 - mu_c1)\n",
    "S_W_2 = (x_p2_c2 - mu_c2).T@(x_p2_c2 - mu_c2)\n",
    "S_W = # fill in your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4:  Find the eigenvectors and eigenvalues of matrix $S_W^{-1}S_B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = # fill in your code here\n",
    "eig_vals, eig_vecs = np.linalg.eig(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eig_vals)):\n",
    "    np.testing.assert_array_almost_equal(W@eig_vecs[:,i].reshape((3,1)), \n",
    "                                         (eig_vals[i] * eig_vecs[:,i]).reshape((3,1)), decimal=6)\n",
    "    \n",
    "print(\"PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Sort the eigenvectors by decreasing eigenvalues, and select top $r$ eigenvectors to construct a $r$-dimensional subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "eig_pairs = sorted(eig_pairs, key=lambda k:k[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eig_pairs)):\n",
    "    print(\"Eigenvalue:\",eig_pairs[i][0].round(3), \"; the corresponding eigenvector:\", eig_pairs[i][1], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The best 1-dimensional subspace is the first eigenvector after sort, and the best 2-dimensional subspace the span of the first two eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Transform the data points onto the new subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no code to fill in in this step. Use the plotting code given to confirm that the LDA gives the best low-dimensional subspace for class separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def visualize_1D_linear_discriminant(ld1, y, n_class, color=[\"red\", \"blue\", \"green\"], marker=[\"s\",\"o\",\"^\"]):\n",
    "    for i in range(n_class):\n",
    "        plt.scatter(ld1[y == i], np.zeros_like(ld1[y == i]), marker=marker[i], color=color[i], alpha=0.5, label=\"class {0}\".format(i))\n",
    "        mean = np.mean(ld1[y == i])\n",
    "        std = np.std(ld1[y == i])\n",
    "        a = np.linspace(mean - 3*std, mean + 3*std, 100)\n",
    "        plt.plot(a, stats.norm.pdf(a, mean, std), color = color[i])\n",
    "        plt.scatter(mean, 0, color=\"black\", marker=marker[i], s=90, label=\"m_{0}\".format(i))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def visualize_2D_linear_discriminants(ld1, y, n_class, color=[\"red\", \"blue\", \"green\"], marker=[\"s\",\"o\",\"^\"]):\n",
    "    for i in range(n_class):\n",
    "        plt.scatter(ld1[:,0][y == i], ld1[:,1][y == i], marker=marker[i], color=color[i], alpha=0.5, label=\"class {0}\".format(i))\n",
    "        mean = np.mean(ld1[y == i], axis = 0)\n",
    "        plt.scatter(mean[0], mean[1], color=\"black\", marker=marker[i], s=70, label=\"m_{0}\".format(i))\n",
    "    mean = np.mean(ld1, axis = 0)\n",
    "    plt.scatter(mean[0], mean[1], color=\"black\", marker=\"x\", s=70, label=\"m\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 dimensional:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code visualize the distribution of the data when projected onto different 1-dimensional subspaces. Apart from the data points, we have also plotted the mean of each class after projection, and a normal distribution curve using each class's mean and variance after projection. \n",
    "\n",
    "By comparing different plots, you may get a sense that when data are projected onto subspaces constructed using LDA, they are more separated (\"high difference in mean and low variances in each class\"). Be cautious that the scale of the x-axis may change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project on the first eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_best = eig_pairs[0][1].reshape((3,1))\n",
    "x_p2_best_1d = x_p2@w_best\n",
    "visualize_1D_linear_discriminant(x_p2_best_1d, y_p2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project on the second eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_second = eig_pairs[1][1].reshape((3,1))\n",
    "x_p2_second_1d = x_p2@w_second\n",
    "x_p2_second_1d\n",
    "visualize_1D_linear_discriminant(x_p2_second_1d, y_p2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project on a arbitrary eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An arbitrary vector w_arbi. You can change this vector and explore. \n",
    "# ** Cautious: the scale of the x-axis may change **\n",
    "w_arbi = np.array([[0],[1],[1]])\n",
    "x_p2_arbi_1d = x_p2@w_arbi\n",
    "visualize_1D_linear_discriminant(x_p2_arbi_1d, y_p2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 dimensional:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code visualize the distribution of the data when projected onto different 2-dimensional subspaces. We have also plotted the mean of each class after projection. We did not construct a multi-variate Gaussian distribution as in the 1-dimensional case, but the variance of each class can be captured by \"how tight the clusters of the classes are\".   \n",
    "\n",
    "Be cautious that the scale of the x-axis and y-axis may change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project on the span of first two eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_best_2d = np.hstack((eig_pairs[0][1].reshape((3,1)), eig_pairs[1][1].reshape((3,1))))\n",
    "x_p2_best_2d = x_p2@W_best_2d\n",
    "visualize_2D_linear_discriminants(x_p2_best_2d, y_p2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project on the span of two arbitrary linearly independent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An arbitrary matrix W_arbi_2d. You can change this vector and explore. \n",
    "# ** Cautious: the scale of the x-axis and y-axis may change **\n",
    "W_arbi_2d = np.array([[1,1],[0,1],[0,1]])\n",
    "x_p2_arbi_2d = x_p2@W_arbi_2d\n",
    "visualize_2D_linear_discriminants(x_p2_arbi_2d, y_p2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations on finishing Activity 2. Hope you now understand how to implement multi-class LDA and get a sense of how it select linear discriminants. By far you have completed the majority of this tutorial. There is one more simple activity that compare LDA and PCA as dimensionality reduction techniques. Hope you will enjoy it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Activity 3 \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "x_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "klass = iris.target_names\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0: 'Setosa', 1: 'Versicolor', 2:'Virginica'}\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "print(xtrain.shape)\n",
    "\n",
    "xtrain_normalized = normalize(xtrain, norm='l2', axis=0)\n",
    "xtest_normalized = normalize(xtest, norm='l2', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2d(X, y, xlab='PC1', ylab='PC2', title='PCA: first 2 principal components'):\n",
    "    ax = plt.subplot(111)\n",
    "    for label,marker,color in zip(\n",
    "        range(0,3),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    "\n",
    "        plt.scatter(x=X[:,0][y == label],\n",
    "                y=X[:,1][y == label],\n",
    "                marker=marker,\n",
    "                color=color,\n",
    "                alpha=0.5,\n",
    "                label=label_dict[label]\n",
    "                )\n",
    "\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    leg = plt.legend(loc=0, fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.title(title)\n",
    "\n",
    "    # hide axis ticks\n",
    "    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    plt.tight_layout\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, xtrain, ytrain, xtest, ytest, model_name):\n",
    "    pred_train = model.predict(xtrain)\n",
    "    err_train = np.mean(pred_train != ytrain)\n",
    "    print(\"The error rate for training set using {:} (r=2) is {:.3f}%\".format(model_name, err_train*100))\n",
    "    \n",
    "    pred_test = model.predict(xtest)\n",
    "    err_test = np.mean(pred_test != ytest)\n",
    "    print(\"The error rate for test set using {:} (r=2) is {:.3f}%\".format(model_name, err_test*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Most Significant Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linalg.inv(xtrain.T@xtrain)@xtrain.T@ytrain\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the last two columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_truncated = xtrain[:, 2:]\n",
    "xtest_truncated = xtest[:, 2:]\n",
    "print(xtrain_truncated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2d(xtrain_truncated, ytrain, x_columns[2], x_columns[3], 'Truncated Dataset with Two Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2d(xtest_truncated, ytest, x_columns[2], x_columns[3], 'Truncated Dataset with Two Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_truncated = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "lr_truncated.fit(xtrain_truncated, ytrain)\n",
    "evaluate_model(lr_truncated, xtrain_truncated, ytrain, xtest_truncated, ytest, \"Truncated X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "xtrain_pca = pca.fit_transform(xtrain_normalized)\n",
    "xtest_pca = pca.transform(xtest_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2d(xtrain_pca, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2d(xtest_pca, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pca = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "lr_pca.fit(xtrain_pca, ytrain)\n",
    "evaluate_model(lr_pca, xtrain_pca, ytrain, xtest_pca, ytest, \"PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda2 = LinearDiscriminantAnalysis(n_components=2, store_covariance=True)\n",
    "xtrain_lda2 = lda2.fit_transform(xtrain_normalized, ytrain)\n",
    "xtest_lda2 = lda2.fit_transform(xtest_normalized, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2d(xtrain_lda2, ytrain, title='LDA: first 2 linear discriminant for the training set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2d(xtest_lda2, ytest, title='LDA: first 2 linear discriminant for the test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lda = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "lr_lda.fit(xtrain_lda2, ytrain)\n",
    "evaluate_model(lr_lda, xtrain_lda2, ytrain, xtest_lda2, ytest, \"LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
